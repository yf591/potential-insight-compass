"""
Data Processor Module - Data Processing Utilities

This module handles data processing, validation, and transformation
for the Potential Insight Compass system.
"""

import json
import re
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict

import pandas as pd


@dataclass
class ProcessedData:
    """Data class for processed analysis data."""

    timestamp: str
    input_text: str
    input_length: int
    strengths: List[str]
    potential_jobs: List[Dict[str, str]]
    scores_df: pd.DataFrame
    processing_time: float
    metadata: Dict[str, Any]


class DataProcessor:
    """
    Data processor class for handling various data operations.

    Features:
    - Text preprocessing and normalization
    - Data validation and cleaning
    - DataFrame creation for visualization
    - Export functionality
    """

    def __init__(self):
        """Initialize the data processor."""
        self.capability_dimensions = [
            "ç¶™ç¶šãƒ»é›†ä¸­åŠ›",
            "å®Ÿè¡Œãƒ»è¡Œå‹•åŠ›",
            "å…±æ„Ÿãƒ»å”èª¿æ€§",
            "è«–ç†ãƒ»åˆ†æåŠ›",
            "å‰µé€ ãƒ»ç™ºæƒ³åŠ›",
            "è¨ˆç”»ãƒ»å …å®Ÿæ€§",
        ]

    def preprocess_text(self, text: str) -> str:
        """
        Preprocess input text for analysis.

        Args:
            text: Raw input text

        Returns:
            Preprocessed text
        """
        if not text:
            return ""

        # Normalize whitespace and line breaks
        processed_text = re.sub(r"\s+", " ", text.strip())

        # Remove excessive punctuation
        processed_text = re.sub(r"[ï¼]{2,}", "ï¼", processed_text)
        processed_text = re.sub(r"[ï¼Ÿ]{2,}", "ï¼Ÿ", processed_text)
        processed_text = re.sub(r"[ã€‚]{2,}", "ã€‚", processed_text)

        # Normalize quotation marks
        processed_text = processed_text.replace('"', '"').replace('"', '"')
        processed_text = processed_text.replace(""", "'").replace(""", "'")

        return processed_text

    def validate_analysis_data(self, data: Dict) -> Tuple[bool, List[str]]:
        """
        Validate analysis data structure and content.

        Args:
            data: Analysis data dictionary

        Returns:
            Tuple of (is_valid, list_of_errors)
        """
        errors = []

        # Check top-level structure
        if not isinstance(data, dict):
            errors.append("ãƒ‡ãƒ¼ã‚¿ãŒè¾æ›¸å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
            return False, errors

        # Check qualitative analysis
        if "qualitative_analysis" not in data:
            errors.append("å®šæ€§åˆ†æãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        else:
            qual_data = data["qualitative_analysis"]

            # Check strengths
            if "strengths" not in qual_data:
                errors.append("å¼·ã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            elif not isinstance(qual_data["strengths"], list):
                errors.append("å¼·ã¿ãƒ‡ãƒ¼ã‚¿ãŒãƒªã‚¹ãƒˆå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
            elif len(qual_data["strengths"]) != 5:
                errors.append(
                    f"å¼·ã¿ã¯5é …ç›®ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆç¾åœ¨: {len(qual_data['strengths'])}é …ç›®ï¼‰"
                )

            # Check potential jobs
            if "potential_jobs" not in qual_data:
                errors.append("è·æ¥­é©æ€§ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            elif not isinstance(qual_data["potential_jobs"], list):
                errors.append("è·æ¥­é©æ€§ãƒ‡ãƒ¼ã‚¿ãŒãƒªã‚¹ãƒˆå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
            elif len(qual_data["potential_jobs"]) != 3:
                errors.append(
                    f"è·æ¥­é©æ€§ã¯3é …ç›®ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆç¾åœ¨: {len(qual_data['potential_jobs'])}é …ç›®ï¼‰"
                )
            else:
                for i, job in enumerate(qual_data["potential_jobs"]):
                    if not isinstance(job, dict):
                        errors.append(f"è·æ¥­é©æ€§{i+1}ãŒè¾æ›¸å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
                    elif "job_title" not in job or "reason" not in job:
                        errors.append(
                            f"è·æ¥­é©æ€§{i+1}ã«job_titleã¾ãŸã¯reasonãŒä¸è¶³ã—ã¦ã„ã¾ã™"
                        )

        # Check quantitative scores
        if "quantitative_scores" not in data:
            errors.append("å®šé‡åˆ†æãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        else:
            scores = data["quantitative_scores"]
            if not isinstance(scores, dict):
                errors.append("å®šé‡åˆ†æãƒ‡ãƒ¼ã‚¿ãŒè¾æ›¸å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
            else:
                for dimension in self.capability_dimensions:
                    if dimension not in scores:
                        errors.append(f"èƒ½åŠ›æ¬¡å…ƒ '{dimension}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    else:
                        score = scores[dimension]
                        if not isinstance(score, (int, float)):
                            errors.append(f"'{dimension}' ã®ã‚¹ã‚³ã‚¢ãŒæ•°å€¤ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
                        elif score < 1 or score > 10:
                            errors.append(
                                f"'{dimension}' ã®ã‚¹ã‚³ã‚¢ãŒç¯„å›²å¤–ã§ã™ï¼ˆ1-10ã®é–“ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼‰"
                            )

        return len(errors) == 0, errors

    def create_scores_dataframe(self, scores: Dict[str, int]) -> pd.DataFrame:
        """
        Create a pandas DataFrame from capability scores.

        Args:
            scores: Dictionary of capability scores

        Returns:
            DataFrame with scores for visualization
        """
        # Create DataFrame
        df = pd.DataFrame(
            [
                {"èƒ½åŠ›æ¬¡å…ƒ": dimension, "ã‚¹ã‚³ã‚¢": scores.get(dimension, 0)}
                for dimension in self.capability_dimensions
            ]
        )

        # Add additional columns for visualization
        df["æœ€å¤§å€¤"] = 10
        df["ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸"] = (df["ã‚¹ã‚³ã‚¢"] / 10) * 100

        return df

    def process_analysis_result(
        self,
        input_text: str,
        analysis_result: Any,
        additional_metadata: Optional[Dict] = None,
    ) -> ProcessedData:
        """
        Process complete analysis result into structured format.

        Args:
            input_text: Original input text
            analysis_result: Analysis result object
            additional_metadata: Additional metadata to include

        Returns:
            ProcessedData object with structured data
        """
        # Create scores DataFrame
        scores_df = self.create_scores_dataframe(analysis_result.quantitative_scores)

        # Prepare metadata
        metadata = {
            "analysis_timestamp": datetime.now().isoformat(),
            "input_character_count": len(input_text),
            "processing_time_seconds": analysis_result.processing_time,
            "api_response_length": len(analysis_result.raw_response),
        }

        if additional_metadata:
            metadata.update(additional_metadata)

        return ProcessedData(
            timestamp=datetime.now().isoformat(),
            input_text=self.preprocess_text(input_text),
            input_length=len(input_text),
            strengths=analysis_result.strengths,
            potential_jobs=analysis_result.potential_jobs,
            scores_df=scores_df,
            processing_time=analysis_result.processing_time,
            metadata=metadata,
        )

    def export_to_json(
        self, processed_data: ProcessedData, include_raw_text: bool = False
    ) -> str:
        """
        Export processed data to JSON format.

        Args:
            processed_data: ProcessedData object to export
            include_raw_text: Whether to include raw input text

        Returns:
            JSON string representation
        """
        export_dict = {
            "timestamp": processed_data.timestamp,
            "input_length": processed_data.input_length,
            "processing_time": processed_data.processing_time,
            "strengths": processed_data.strengths,
            "potential_jobs": processed_data.potential_jobs,
            "quantitative_scores": processed_data.scores_df[["èƒ½åŠ›æ¬¡å…ƒ", "ã‚¹ã‚³ã‚¢"]]
            .set_index("èƒ½åŠ›æ¬¡å…ƒ")["ã‚¹ã‚³ã‚¢"]
            .to_dict(),
            "metadata": processed_data.metadata,
        }

        if include_raw_text:
            export_dict["input_text"] = processed_data.input_text

        return json.dumps(export_dict, ensure_ascii=False, indent=2)

    def export_to_markdown(self, processed_data: ProcessedData) -> str:
        """
        Export processed data to Markdown format.

        Args:
            processed_data: ProcessedData object to export

        Returns:
            Markdown string representation
        """
        md_content = f"""# æ½œåœ¨èƒ½åŠ›åˆ†æçµæœãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ“Š åˆ†ææ¦‚è¦

- **åˆ†ææ—¥æ™‚**: {processed_data.timestamp}
- **å…¥åŠ›æ–‡å­—æ•°**: {processed_data.input_length:,} æ–‡å­—
- **å‡¦ç†æ™‚é–“**: {processed_data.processing_time:.2f} ç§’

## ğŸ’ª ç™ºè¦‹ã•ã‚ŒãŸå¼·ã¿

"""

        for i, strength in enumerate(processed_data.strengths, 1):
            md_content += f"{i}. {strength}\n"

        md_content += "\n## ğŸ¯ é©æ€§ã®ã‚ã‚‹è·æ¥­\n\n"

        for i, job in enumerate(processed_data.potential_jobs, 1):
            md_content += f"### {i}. {job['job_title']}\n\n"
            md_content += f"**ç†ç”±**: {job['reason']}\n\n"

        md_content += "## ğŸ“ˆ èƒ½åŠ›ã‚¹ã‚³ã‚¢\n\n"

        for _, row in processed_data.scores_df.iterrows():
            dimension = row["èƒ½åŠ›æ¬¡å…ƒ"]
            score = row["ã‚¹ã‚³ã‚¢"]
            percentage = row["ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸"]
            bar = "â–ˆ" * int(percentage // 10) + "â–‘" * (10 - int(percentage // 10))
            md_content += f"**{dimension}**: {score}/10 `{bar}` ({percentage:.0f}%)\n\n"

        return md_content

    def calculate_statistics(self, scores_df: pd.DataFrame) -> Dict[str, float]:
        """
        Calculate statistical measures from scores.

        Args:
            scores_df: DataFrame with capability scores

        Returns:
            Dictionary with statistical measures
        """
        scores = scores_df["ã‚¹ã‚³ã‚¢"].values

        return {
            "å¹³å‡å€¤": float(scores.mean()),
            "æœ€å¤§å€¤": float(scores.max()),
            "æœ€å°å€¤": float(scores.min()),
            "æ¨™æº–åå·®": float(scores.std()),
            "ä¸­å¤®å€¤": float(pd.Series(scores).median()),
            "åˆè¨ˆå€¤": float(scores.sum()),
            "ãƒ¬ãƒ³ã‚¸": float(scores.max() - scores.min()),
        }

    def identify_top_strengths(
        self, scores_df: pd.DataFrame, top_n: int = 3
    ) -> List[Tuple[str, int]]:
        """
        Identify top N capability dimensions based on scores.

        Args:
            scores_df: DataFrame with capability scores
            top_n: Number of top capabilities to return

        Returns:
            List of tuples (dimension_name, score) sorted by score
        """
        sorted_scores = scores_df.sort_values("ã‚¹ã‚³ã‚¢", ascending=False)
        return [
            (row["èƒ½åŠ›æ¬¡å…ƒ"], row["ã‚¹ã‚³ã‚¢"])
            for _, row in sorted_scores.head(top_n).iterrows()
        ]

    def identify_development_areas(
        self, scores_df: pd.DataFrame, bottom_n: int = 2
    ) -> List[Tuple[str, int]]:
        """
        Identify areas for development based on lowest scores.

        Args:
            scores_df: DataFrame with capability scores
            bottom_n: Number of development areas to return

        Returns:
            List of tuples (dimension_name, score) sorted by score (lowest first)
        """
        sorted_scores = scores_df.sort_values("ã‚¹ã‚³ã‚¢", ascending=True)
        return [
            (row["èƒ½åŠ›æ¬¡å…ƒ"], row["ã‚¹ã‚³ã‚¢"])
            for _, row in sorted_scores.head(bottom_n).iterrows()
        ]
